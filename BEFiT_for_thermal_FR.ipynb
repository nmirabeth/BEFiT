{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94118403",
   "metadata": {},
   "source": [
    "## 0. Load some necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011917a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from transformers import BeitImageProcessor\n",
    "from torchvision.transforms import (CenterCrop, \n",
    "                                    Compose, \n",
    "                                    Normalize, \n",
    "                                    RandomHorizontalFlip,\n",
    "                                    RandomResizedCrop, \n",
    "                                    Resize, \n",
    "                                    ToTensor)\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b435a10d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the GPU\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50303242",
   "metadata": {},
   "source": [
    "## 1. Face detection and cropping"
   ]
  },
  {
   "cell_type": "raw",
   "id": "db3d4215",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "#from mtcnn import MTCNN\n",
    "import cv2\n",
    "import dlib\n",
    "from face_detection import crop_faces\n",
    "\n",
    "# Parameters\n",
    "MARGIN = 0\n",
    "sign='pos'\n",
    "detector='dlib'\n",
    "not_crop_count=[]\n",
    "\n",
    "\n",
    "if detector=='dlib':   \n",
    "    face_detector = dlib.get_frontal_face_detector()#MTCNN()\n",
    "\n",
    "    \n",
    "#for direc in range(113,114): #80:\n",
    "    # Directories info\n",
    "    ORIGINAL_IMGS_DIR = 'TUFTS/TD_IR_E/'+np.str(direc) # To be changed\n",
    "    CROPPED_IMGS_DIR = 'crop_TUFTS/'+np.str(direc)\n",
    "    \n",
    "    if os.path.exists(ORIGINAL_IMGS_DIR) == False:\n",
    "        continue\n",
    "\n",
    "    counter=crop_faces(detector, face_detector, MARGIN, ORIGINAL_IMGS_DIR, CROPPED_IMGS_DIR, sign, plot_images=False, face_align=False)\n",
    "    not_crop_count.append(counter)\n",
    "    \n",
    "np.sum(not_crop_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d37af7a",
   "metadata": {},
   "source": [
    "## Load the cropped dataset (TUFTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93544956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#load TUFTS\n",
    "train_ds = load_dataset(\"imagefolder\", data_dir=\"crop_TUFTS\", split=\"train\")\n",
    "np.shape(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f6b07b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's print out the dataset:\n",
    "print('Dataset info:' ,train_ds)\n",
    "\n",
    "# We can also check out the features of the dataset in more detail:\n",
    "print('Dataset features: ', train_ds.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3230900",
   "metadata": {},
   "source": [
    "## 2. Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ac6a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BeitImageProcessor\n",
    "\n",
    "# Load the pre-trained model: Self-supervised on ImageNet-22k (14 million images, 21,841 classes) at resolution 224x224\n",
    "# and fine-tuned on the same dataset at resolution 224x224.\n",
    "processor = BeitImageProcessor.from_pretrained(\"microsoft/beit-base-patch16-224-pt22k-ft22k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce7032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (CenterCrop, \n",
    "                                    Compose, \n",
    "                                    Normalize, \n",
    "                                    RandomHorizontalFlip,\n",
    "                                    RandomResizedCrop, \n",
    "                                    Resize, \n",
    "                                    ToTensor)\n",
    "\n",
    "image_mean, image_std = processor.image_mean, processor.image_std\n",
    "size = processor.size[\"height\"]\n",
    "\n",
    "normalize = Normalize(mean=image_mean, std=image_std)\n",
    "_train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "_val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def train_transforms(examples):\n",
    "    examples['pixel_values'] = [_train_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "def val_transforms(examples):\n",
    "    examples['pixel_values'] = [_val_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceeee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the transforms\n",
    "train_ds.set_transform(train_transforms)\n",
    "val_ds.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987d1c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# Create a corresponding PyTorch DataLoader\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, collate_fn=collate_fn, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9db7bc5",
   "metadata": {},
   "source": [
    "## 3. Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bcea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BeitForImageClassification\n",
    "\n",
    "model = BeitForImageClassification.from_pretrained('trainer') # Pre-trained BEFiT-V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d113c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "metric_name = \"accuracy\"\n",
    "\n",
    "# We define the class `TrainingArguments` containing all the attributes to customize the training. \n",
    "# It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional.\n",
    "args = TrainingArguments(\n",
    "    f\"test_transformers_thermal\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=150,\n",
    "    weight_decay=0.05, # Regularization that penalizes large weights. Adds a term to the loss function proportional to the sum of the squared weights. Prevents the weights from growing too large.\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    logging_dir='logs_thermal',\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "# Here we set the evaluation to be done at the end of each epoch,\n",
    "# tweak the learning rate, set the training and evaluation batch_sizes and\n",
    "# customize the number of epochs for training, as well as the weight decay.\n",
    "# We also set the argument \"remove_unused_columns\" to False, because otherwise the \"image\" column would be removed, \n",
    "# which is required for the data transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f1875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# We also define a `compute_metrics` function that will be used to compute metrics at evaluation. We use \"accuracy\" here.\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return dict(accuracy=accuracy_score(predictions, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d701e9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass all the information to the trainer\n",
    "trainer_thermal = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe21a4ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can now finetune our model by just calling the `train` method:\n",
    "trainer_thermal.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1677a748",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_thermal.save_model(\"trainer_thermal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e68ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
